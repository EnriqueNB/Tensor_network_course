{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653c1817",
   "metadata": {},
   "source": [
    "# Generic tensor contractions\n",
    "\n",
    "Almost literally copied from  G. Evenbly tensors.net tutorial 1\n",
    "\n",
    "## Defining generic tensors  \n",
    "Here we will start studying generic tensor networks and their contraction.\n",
    "Part of any tensor network study is to understand how to contract some of the parts of the tensor networks, and eventually simplify them. \n",
    "\n",
    "The first task is to initialize tensors, we typically did this by creating vectors and reshaping them into higher dimensional tensors, today we will start with those higher dimensional tensors.\n",
    "Let' s create a random tensor with three legs (an order 4 tensor) with size 2, 3, 4, 5 and call it A, remember we always work with complex tensors in quantum mechanics\n",
    "and an order 3 tensor with size 4 5 6 and call it B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de370d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b8738da",
   "metadata": {},
   "source": [
    "Then we create an identity matrix that is 5 by 5 and call it C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b2333a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d038b4ea",
   "metadata": {},
   "source": [
    "Other special tensors are those made by all ones, that clearly can be multiplied by an arbitrary random complex constant, thus obtaining a tensor where each element is the same, create one that has order 4 and dimension 2, 4, 2, 4, create it and call it D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a40856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "621280c1",
   "metadata": {},
   "source": [
    "Finally when a tensor only has few non-zero elements, one can create a tensor made of all zeros and fill the desider elements. E.g. create a tensor with order 2 made of zero and fill the first element with a random complex number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c1024b",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a1fb2",
   "metadata": {},
   "source": [
    "Now we can reorder the legs (permuting them which incurs in a computational cost proportional to the size of the tensor) or grouping or splitting the legs, which does not have a relevant computational cost (for large tensors) since it only changes the labels used to address the elements ![reshape_permute](../pictures/reshape_permute.png \"from G. Evenbly\")\n",
    "For example implement the above permutation and reshaping  for the tensor A  and B defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2815e657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f1b5a91",
   "metadata": {},
   "source": [
    "## Tensor contractions\n",
    "\n",
    "We now enter the realm of tensor contractions. First of all  remeber that contracting two tensors means summing the product of the tensor elements, it is a generalization of matrix multiplication\n",
    "\n",
    "$ M^i_k =\\sum _j A^i_j * B^j_k$, in the theory material you have gone through the diagramatic (or Penrose) notation for such operations. So let's put them in practice here. \n",
    "\n",
    "We will start by contracting two tensors. This can be done in several ways.\n",
    "Define a new tensor $B$ with order 4 and dimensions 3,4,2,5\n",
    "Now contract it with the $A$ tensor defined above on the second and fifth leg\n",
    "![Contraction between two tensors](../pictures/two_tensors_contract.png \"For G. Evenbly\")\n",
    "\n",
    "First compute it using for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec68ed0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccaf13fa",
   "metadata": {},
   "source": [
    "Now repeat the same operation by transforming the two tensors into matrix and then performing a matrix multiplication, call the resulting tensor $\\tilde{C}$ and compare that the two methods provide the same result.\n",
    "![Contraction as matrix multiplication](../pictures/contraction_as_matrix_multiplication.png \"For G. Evenbly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e8e6041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5543122344752192e-15-1.7763568394002505e-15j)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4d314",
   "metadata": {},
   "source": [
    "## Computational cost of tensor contractions. \n",
    "\n",
    "As you have seen in the theory material, contracting tensors comes at a cost, here there is the summary of that cost is in this picture from tensors.net \n",
    "![Cost of contraction](../pictures/contraction_cost.png \"from G. Evenbly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523bd6d1",
   "metadata": {},
   "source": [
    "## More than 2 tensors \n",
    "When your network to contract include more than two tensors it is computationally advantageous to break the contraction into pairwise contractions. For example if you need to contract three tensors, and you do it in a single shot (by using for loops) you incur into a higher computational cost. Try it below follwing the diagarm. Contract it with for loops and by sequence of matrix multiplications.\n",
    "![three_tensors](../pictures/three_tensors.png \"from G. Evenbly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b8df526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60d93eeb",
   "metadata": {},
   "source": [
    "In general the cost of the contraction of a tensor network depends on the order of the contraction. The picture below from tensors.net provide an explicit example \n",
    "![Cost of contraction](../pictures/order_cost.png \"from G. Evenbly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070447bf",
   "metadata": {},
   "source": [
    "As an exercise find the optimal contraction sequence of this diagarm (from tensors.net) and write the code the performs the optimized contraction transforming the tensors to matrices and multiplying them.\n",
    " ![reshape_permute](../pictures/exercice.png \"from G. Evenbly\")\n",
    "  Initialize the tensors as random tensors whose legs all share the same dimension $d=10$ \n",
    "  use the following convention for the ordering of indices\n",
    "   ![reshape_permute](../pictures/oredering_indices.png \"from G. Evenbly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87a6e8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf53df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
